{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import glob\n",
    "import os\n",
    "from pyexpat import ExpatError\n",
    "from xml.dom import minidom\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "\n",
    "STOP_WORDS = set(stopwords.words('english')) | set('the')\n",
    "\n",
    "pd.set_option('display.width', 1000)\n",
    "dataset_csv_file = 'dataset_dataframe.csv'\n",
    "types = set()\n",
    "\n",
    "training_dataset_dataframe = None\n",
    "\n",
    "\n",
    "def get_entity_dict(sentence_dom):\n",
    "    entities = sentence_dom.getElementsByTagName('entity')\n",
    "    entity_dict = {}\n",
    "    for entity in entities:\n",
    "        id = entity.getAttribute('id')\n",
    "        word = entity.getAttribute('text')\n",
    "        entity_dict[id] = word\n",
    "    return entity_dict\n",
    "\n",
    "\n",
    "def normalize_sentence(row):\n",
    "    sentence = row.sentence_text.replace('.', ' . ')\n",
    "    sentence = sentence.replace(',', ' , ')\n",
    "    e1 = row.e1\n",
    "    e2 = row.e2\n",
    "    new_sentence_tokenized = []\n",
    "    i = 0\n",
    "    for word in sentence.split():\n",
    "        if word in STOP_WORDS:\n",
    "            continue\n",
    "        if word.lower() == e1.lower():\n",
    "            new_sentence_tokenized.append('DRUG')\n",
    "            i += 1\n",
    "        elif word.lower() == e2.lower():\n",
    "            new_sentence_tokenized.append('OTHER_DRUG')\n",
    "            i += 1\n",
    "        elif i == 0:\n",
    "            new_sentence_tokenized.append(word + '_bf')\n",
    "        elif i == 1:\n",
    "            new_sentence_tokenized.append(word + '_be')\n",
    "        else:\n",
    "            new_sentence_tokenized.append(word + '_af')\n",
    "    normalized_sentence = ' '.join(new_sentence_tokenized).strip()\n",
    "    # print(e1, e2, ' :  sentence :', sentence, 'new_sentence', normalized_sentence, '\\n\\n')\n",
    "    return normalized_sentence\n",
    "\n",
    "\n",
    "def get_dataset_dataframe(directory=None):\n",
    "    global training_dataset_dataframe, dataset_csv_file\n",
    "\n",
    "    if training_dataset_dataframe:\n",
    "        return training_dataset_dataframe\n",
    "    global types\n",
    "\n",
    "    if directory is None:\n",
    "        directory = os.path.expanduser('E:/VIT/RBL/ddi/dataset/DDICorpus/Train/DrugBank/')\n",
    "\n",
    "    dataset_csv_file_prefix = str(directory.split('/')[-3]).lower() + '_'\n",
    "\n",
    "    dataset_csv_file = dataset_csv_file_prefix + dataset_csv_file\n",
    "    if os.path.isfile(dataset_csv_file):\n",
    "        df = pd.read_csv(dataset_csv_file)\n",
    "        return df\n",
    "\n",
    "    lol = []\n",
    "    total_files_to_read = glob.glob(directory + '*.xml')\n",
    "    print('total_files_to_read:' , len(total_files_to_read) , ' from dir: ' , directory)\n",
    "    for file in tqdm(total_files_to_read):\n",
    "        try:\n",
    "            DOMTree = minidom.parse(file)\n",
    "            sentences = DOMTree.getElementsByTagName('sentence')\n",
    "\n",
    "            for sentence_dom in sentences:\n",
    "                entity_dict = get_entity_dict(sentence_dom)\n",
    "\n",
    "                pairs = sentence_dom.getElementsByTagName('pair')\n",
    "                sentence_text = sentence_dom.getAttribute('text')\n",
    "                for pair in pairs:\n",
    "                    ddi_flag = pair.getAttribute('ddi')\n",
    "                    print(pair.attributes().items())\n",
    "                    if not os.path.isfile('types'):\n",
    "                        types.add(pair.getAttribute('type'))\n",
    "                    if ddi_flag == 'true':\n",
    "                        e1 = pair.getAttribute('e1')\n",
    "                        e2 = pair.getAttribute('e2')\n",
    "                        relation_type = pair.getAttribute('type')\n",
    "                        lol.append([sentence_text, entity_dict[e1], entity_dict[e2], relation_type])\n",
    "        except ExpatError:\n",
    "            pass\n",
    "\n",
    "    pd.to_pickle(types, 'types')\n",
    "    df = pd.DataFrame(lol, columns='sentence_text,e1,e2,relation_type'.split(','))\n",
    "    df['normalized_sentence'] = df.apply(normalize_sentence, axis=1)\n",
    "    df.to_csv(dataset_csv_file)\n",
    "    df = pd.read_csv(dataset_csv_file)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_training_label(row):\n",
    "    global types\n",
    "\n",
    "    types = pd.read_pickle('types')\n",
    "    types = [t for t in types if t]\n",
    "    type_list = list(types)\n",
    "    relation_type = row.relation_type\n",
    "    X = [i for i, t in enumerate(type_list) if relation_type == t]\n",
    "    # s = np.sum(X)\n",
    "    if X:\n",
    "        return X[0]\n",
    "    else:\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from dataset.read_dataset import get_dataset_dataframe\n",
    "from grammar.chunker import Chunker\n",
    "from grammar.syntactic_grammar import PatternGrammar\n",
    "\n",
    "frequent_word_pairs = None\n",
    "K = 200\n",
    "import pandas as pd\n",
    "\n",
    "from spacy.lang.en import English\n",
    "\n",
    "parser = English()\n",
    "import os\n",
    "\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def get_dataset_dictionary():\n",
    "    top_post_fixed_word_file = 'top_post_fixed_word.pkl'\n",
    "    if os.path.isfile(top_post_fixed_word_file):\n",
    "        return pd.read_pickle(top_post_fixed_word_file)\n",
    "    df = get_dataset_dataframe()\n",
    "    word_counter = Counter()\n",
    "    for _, row in df.iterrows():\n",
    "        unique_tokens = sorted(set(word for word in row.normalized_sentence.split()))\n",
    "        # exclude duplicates in same line and sort to ensure one word is always before other\n",
    "        bi_grams = ngrams(row.normalized_sentence.split(), 2)\n",
    "        word_counter += Counter([' '.join(bi_gram).strip() for bi_gram in bi_grams])\n",
    "        word_counter += Counter(unique_tokens)\n",
    "    frequent_words = sorted(list(dict(word_counter.most_common(100000)).keys()))  # return the actual Counter object\n",
    "    pd.to_pickle(frequent_words, top_post_fixed_word_file)\n",
    "    return frequent_words\n",
    "\n",
    "\n",
    "def extract_top_word_pair_features():\n",
    "    frequent_phrase_pickle_path = 'frequent_phrase.pkl'\n",
    "    if not os.path.isfile(frequent_phrase_pickle_path):\n",
    "        df = get_dataset_dataframe()\n",
    "        pair_counter = Counter()\n",
    "        for _, row in df.iterrows():\n",
    "\n",
    "            unique_tokens = sorted(set(word for word in row.normalized_sentence.split()))\n",
    "            # exclude duplicates in same line and sort to ensure one word is always before other\n",
    "            combos = combinations(unique_tokens, 2)\n",
    "            pair_counter += Counter(combos)\n",
    "\n",
    "        frequent_phrase = sorted(list(dict(pair_counter.most_common(K)).keys()))  # return the actual Counter object\n",
    "        pd.to_pickle(frequent_phrase, frequent_phrase_pickle_path)\n",
    "    else:\n",
    "        frequent_phrase = pd.read_pickle(frequent_phrase_pickle_path)\n",
    "    print('frequent_phrase: ' , frequent_phrase[:5])\n",
    "    return frequent_phrase\n",
    "\n",
    "\n",
    "def extract_top_syntactic_grammar_trio():\n",
    "    top_syntactic_grammar_trio_file = 'top_syntactic_grammar_trio_file.pkl'\n",
    "    if os.path.isfile(top_syntactic_grammar_trio_file):\n",
    "        return pd.read_pickle(top_syntactic_grammar_trio_file)\n",
    "\n",
    "    df = get_dataset_dataframe()\n",
    "    trio_counter = Counter()\n",
    "    for _, row in df.iterrows():\n",
    "        combos = extract_syntactic_grammar(row.sentence_text)\n",
    "        trio_counter += Counter(combos)\n",
    "\n",
    "    frequent_trio_counter = sorted(list(dict(trio_counter.most_common(K)).keys()))  # return the actual Counter object\n",
    "    pd.to_pickle(frequent_trio_counter, top_syntactic_grammar_trio_file)\n",
    "    return frequent_trio_counter\n",
    "\n",
    "\n",
    "def extract_dependency_relations(sentence):\n",
    "    # TODO : introduce dependency relation later\n",
    "    parsedEx = parser(sentence)\n",
    "    for token in parsedEx:\n",
    "        print(token.orth_, token.dep_, token.head.orth_)\n",
    "\n",
    "\n",
    "def extract_syntactic_grammar(sentence):\n",
    "    grammar = PatternGrammar().get_syntactic_grammar(0)\n",
    "    chunk_dict = Chunker(grammar).chunk_sentence(sentence)\n",
    "    trigrams_list = []\n",
    "    for key, pos_tagged_sentences in chunk_dict.items():\n",
    "        pos_tags = [token[1] for pos_tagged_sentence in pos_tagged_sentences for token in pos_tagged_sentence]\n",
    "        if len(pos_tags) > 2:\n",
    "            trigrams = ngrams(pos_tags, 3)\n",
    "            trigrams_list = [' '.join(trigram) for trigram in trigrams]\n",
    "\n",
    "    return trigrams_list\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     df = get_dataset_dataframe()\n",
    "#     print(get_dataset_dictionary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import nltk\n",
    "\n",
    "from grammar.pos_tagger import PosTagger\n",
    "\n",
    "\n",
    "\n",
    "class Chunker:\n",
    "    def __init__(self, grammar: nltk.RegexpParser):\n",
    "        self.grammar = grammar\n",
    "\n",
    "    def chunk_sentence(self, sentence: str):\n",
    "        pos_tagged_sentence = PosTagger(sentence).pos_tag()\n",
    "        return dict(self.chunk_pos_tagged_sentence(pos_tagged_sentence))\n",
    "\n",
    "    def chunk_pos_tagged_sentence(self, pos_tagged_sentence):\n",
    "        chunked_tree = self.grammar.parse(pos_tagged_sentence)\n",
    "        chunk_dict = self.extract_rule_and_chunk(chunked_tree)\n",
    "        return chunk_dict\n",
    "\n",
    "    def extract_rule_and_chunk(self, chunked_tree: nltk.Tree) -> dict:\n",
    "        def recursively_get_pos_only(tree, collector_list=None, depth_limit=100):\n",
    "            if collector_list is None:\n",
    "                collector_list = []\n",
    "            if depth_limit <= 0:\n",
    "                return collector_list\n",
    "            for subtree in tree:\n",
    "                if isinstance(subtree, nltk.Tree):\n",
    "                    recursively_get_pos_only(subtree, collector_list, depth_limit - 1)\n",
    "                else:\n",
    "                    collector_list.append(subtree)\n",
    "            return collector_list\n",
    "\n",
    "        def get_pos_tagged_and_append_to_chunk_dict(chunk_dict, subtrees):  # params can be removed now\n",
    "            pos_tagged = recursively_get_pos_only(subtrees)\n",
    "            chunk_dict[subtrees.label()].append(pos_tagged)\n",
    "\n",
    "        chunk_dict = nltk.defaultdict(list)\n",
    "        for subtrees in chunked_tree:\n",
    "            if isinstance(subtrees, nltk.Tree):\n",
    "                get_pos_tagged_and_append_to_chunk_dict(chunk_dict, subtrees)\n",
    "                for sub in subtrees:\n",
    "                    if isinstance(sub, nltk.Tree):\n",
    "                        get_pos_tagged_and_append_to_chunk_dict(chunk_dict, sub)\n",
    "        return chunk_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import nltk\n",
    "from nltk import PerceptronTagger\n",
    "\n",
    "\n",
    "class PosTagger:\n",
    "    def __init__(self, sentence):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            sentence:\n",
    "        \"\"\"\n",
    "        self.sentence = sentence\n",
    "        self.tagger = PosTagger.get_tagger()\n",
    "\n",
    "    def pos_tag(self):\n",
    "        \"\"\"\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        tokens = nltk.word_tokenize(self.sentence)\n",
    "        pos_tagged_tokens = self.tagger.tag(tokens)\n",
    "        return pos_tagged_tokens\n",
    "\n",
    "    @staticmethod\n",
    "    def get_tagger():\n",
    "        \"\"\"\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        return PerceptronTagger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import nltk\n",
    "\n",
    "syntactic_compiled_grammar = {}\n",
    "\n",
    "\n",
    "class PatternGrammar:\n",
    "    @property\n",
    "    def syntactic_grammars(self):\n",
    "        grammar = {\n",
    "            0: \"\"\"\n",
    "                JJ_VBG_RB_DESCRIBING_NN: {   (<CC|,>?<JJ|JJ.>*<VB.|V.>?<NN|NN.>)+<RB|RB.>*<MD>?<WDT|DT>?<VB|VB.>?<RB|RB.>*(<CC|,>?<RB|RB.>?<VB|VB.|JJ.|JJ|RB|RB.>+)+}\n",
    "                \"\"\",\n",
    "            1: \"\"\"\n",
    "                    VBG_DESRIBING_NN: {<NN|NN.><VB|VB.>+<RB|RB.>*<VB|VB.>}\n",
    "                \"\"\",\n",
    "        }\n",
    "        return grammar\n",
    "\n",
    "    def get_syntactic_grammar(self, index):\n",
    "        global syntactic_compiled_grammar\n",
    "        compiled_grammar = syntactic_compiled_grammar.get(index, None)\n",
    "        if compiled_grammar is None:\n",
    "            compiled_grammar = self.compile_syntactic_grammar(index)\n",
    "            syntactic_compiled_grammar[index] = compiled_grammar\n",
    "        return compiled_grammar\n",
    "\n",
    "    def compile_syntactic_grammar(self, index):\n",
    "        return nltk.RegexpParser(self.syntactic_grammars[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from itertools import combinations\n",
    "\n",
    "from nltk import ngrams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "dataset_dictionary = None\n",
    "top_word_pair_features = None\n",
    "top_syntactic_grammar_list = None\n",
    "\n",
    "trained_model_pickle_file = 'trained_model.pkl'\n",
    "\n",
    "\n",
    "def get_empty_vector(n):\n",
    "    return [0 for _ in range(n)]\n",
    "\n",
    "\n",
    "def get_top_word_dataset_dictionary():\n",
    "    from feaure_extraction.feature_vector import get_dataset_dictionary\n",
    "\n",
    "    global dataset_dictionary\n",
    "    if dataset_dictionary is None:\n",
    "        dataset_dictionary = get_dataset_dictionary()\n",
    "    return dataset_dictionary\n",
    "\n",
    "\n",
    "def get_top_word_pair_features():\n",
    "    from feaure_extraction.feature_vector import extract_top_word_pair_features\n",
    "\n",
    "    global top_word_pair_features\n",
    "    if top_word_pair_features is None:\n",
    "        top_word_pair_features = extract_top_word_pair_features()\n",
    "    return top_word_pair_features\n",
    "\n",
    "\n",
    "def get_top_syntactic_grammar_list():\n",
    "    from feaure_extraction.feature_vector import extract_top_syntactic_grammar_trio\n",
    "\n",
    "    global top_syntactic_grammar_list\n",
    "    if top_syntactic_grammar_list is None:\n",
    "        top_syntactic_grammar_list = extract_top_syntactic_grammar_trio()\n",
    "    return top_syntactic_grammar_list\n",
    "\n",
    "\n",
    "def get_word_feature(normalized_sentence):\n",
    "    unique_tokens = set(word for word in normalized_sentence.split())\n",
    "    # exclude duplicates in same line and sort to ensure one word is always before other\n",
    "    bi_grams = set(ngrams(normalized_sentence.split(), 2))\n",
    "    words = unique_tokens | bi_grams\n",
    "    dataset_dictionary = get_top_word_dataset_dictionary()\n",
    "    X = [i if j in words else 0 for i, j in enumerate(dataset_dictionary)]\n",
    "    return X\n",
    "\n",
    "\n",
    "def get_frequent_word_pair_feature(normalized_sentence):\n",
    "    unique_tokens = sorted(set(word for word in normalized_sentence.split()))\n",
    "    # exclude duplicates in same line and sort to ensure one word is always before other\n",
    "    combos = combinations(unique_tokens, 2)\n",
    "    top_word_pair_features = get_top_word_pair_features()\n",
    "    X = [i if j in combos else 0 for i, j in enumerate(top_word_pair_features)]\n",
    "    return X\n",
    "\n",
    "\n",
    "def get_syntactic_grammar_feature(sentence_text):\n",
    "    from feaure_extraction.feature_vector import extract_syntactic_grammar\n",
    "    trigrams_list = extract_syntactic_grammar(sentence_text)\n",
    "    top_syntactic_grammar_list = get_top_syntactic_grammar_list()\n",
    "    X = [i if j in trigrams_list else 0 for i, j in enumerate(top_syntactic_grammar_list)]\n",
    "    return X\n",
    "\n",
    "\n",
    "def make_feature_vector(row):\n",
    "    normalized_sentence = row.normalized_sentence\n",
    "    sentence = row.sentence_text\n",
    "\n",
    "    word_feature = get_word_feature(normalized_sentence)\n",
    "    frequent_word_feature = get_frequent_word_pair_feature(normalized_sentence)\n",
    "    syntactic_grammar_feature = get_syntactic_grammar_feature(sentence)\n",
    "\n",
    "    features = word_feature\n",
    "    features.extend(frequent_word_feature)\n",
    "    features.extend(syntactic_grammar_feature)\n",
    "    return features\n",
    "\n",
    "def main():\n",
    "    from dataset.read_dataset import get_dataset_dataframe\n",
    "    df = get_dataset_dataframe()\n",
    "    X, Y = extract_training_data_from_dataframe(df)\n",
    "    from sklearn.svm import SVC\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, Y, test_size=.2, random_state=42)\n",
    "\n",
    "    print(df.head())\n",
    "    print('X: ', (X.shape), 'Y : ', np.array(Y.shape))\n",
    "    model = SVC(kernel='linear')\n",
    "    model.fit(X_train, y_train)\n",
    "    score = model.score(X_test, y_test)\n",
    "    import pandas as pd\n",
    "\n",
    "    pd.to_pickle(model, trained_model_pickle_file)\n",
    "    classification_report()\n",
    "    print('Score : ', score)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "def extract_training_data_from_dataframe(df):\n",
    "    from dataset.read_dataset import get_training_label\n",
    "\n",
    "    X = df.apply(make_feature_vector, axis=1)\n",
    "    Y = df.apply(get_training_label, axis=1)\n",
    "    X = np.array(X.tolist())\n",
    "    Y = np.array(Y.tolist())\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_files_to_read: 569  from dir:  E:/VIT/RBL/ddi/dataset/DDICorpus/Train/DrugBank/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 569/569 [00:02<00:00, 213.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequent_phrase:  [('(e_be', ',_af'), ('(e_be', ',_be'), ('(e_be', '._af'), ('(e_be', '._be'), ('(e_be', 'DRUG')]\n",
      "   Unnamed: 0                                      sentence_text  \\\n",
      "0           0  Concurrent administration of a TNF antagonist ...   \n",
      "1           1  Concurrent therapy with ORENCIA and TNF antago...   \n",
      "2           2  There is insufficient experience to assess the...   \n",
      "3           3  Co-administration of naltrexone with Acamprosa...   \n",
      "4           4  Patients taking Acamprosate concomitantly with...   \n",
      "\n",
      "               e1               e2 relation_type  \\\n",
      "0  TNF antagonist          ORENCIA        effect   \n",
      "1         ORENCIA  TNF antagonists        advise   \n",
      "2         ORENCIA         anakinra        advise   \n",
      "3      naltrexone      Acamprosate     mechanism   \n",
      "4     Acamprosate  antidepressants        effect   \n",
      "\n",
      "                                 normalized_sentence  \n",
      "0  Concurrent_bf administration_bf TNF_bf antagon...  \n",
      "1  Concurrent_bf therapy_bf DRUG TNF_be antagonis...  \n",
      "2  There_bf insufficient_bf experience_bf assess_...  \n",
      "3  Co-administration_bf DRUG OTHER_DRUG produced_...  \n",
      "4  Patients_bf taking_bf DRUG concomitantly_be OT...  \n",
      "X:  (3006, 26667) Y :  [3006]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The number of classes has to be greater than one; got 1 class",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-70b37cb0f641>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'X: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Y : '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'linear'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    148\u001b[0m                          \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'C'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                          accept_large_sparse=False)\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         sample_weight = np.asarray([]\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_validate_targets\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    523\u001b[0m             raise ValueError(\n\u001b[0;32m    524\u001b[0m                 \u001b[1;34m\"The number of classes has to be greater than one; got %d\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m                 \" class\" % len(cls))\n\u001b[0m\u001b[0;32m    526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The number of classes has to be greater than one; got 1 class"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from dataset.read_dataset import get_dataset_dataframe\n",
    "from training.train import extract_training_data_from_dataframe, trained_model_pickle_file\n",
    "import pandas as pd\n",
    "import os\n",
    "def predict():\n",
    "    df = get_dataset_dataframe(directory=os.path.expanduser('E:/VIT/RBL/ddi/dataset/DDICorpus/Test/test_for_ddi_extraction_task/DrugBank/'))\n",
    "    X, Y = extract_training_data_from_dataframe(df)\n",
    "    model = pd.read_pickle(trained_model_pickle_file)\n",
    "    y_pred  = model.predict(X)\n",
    "\n",
    "    print(classification_report(Y, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_files_to_read: 158  from dir:  E:/VIT/RBL/ddi/dataset/DDICorpus/Test/test_for_ddi_extraction_task/DrugBank/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 158/158 [00:00<00:00, 287.20it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'trained_model.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mtry_read\u001b[1;34m(path, encoding)\u001b[0m\n\u001b[0;32m    165\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mread_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpkl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# noqa: E722\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_wrapper\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m    145\u001b[0m                             \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m                             is_text=False)\n\u001b[0m\u001b[0;32m    147\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;31m# Python 3 and binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'trained_model.pkl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mtry_read\u001b[1;34m(path, encoding)\u001b[0m\n\u001b[0;32m    172\u001b[0m                 return read_wrapper(\n\u001b[1;32m--> 173\u001b[1;33m                     lambda f: pc.load(f, encoding=encoding, compat=False))\n\u001b[0m\u001b[0;32m    174\u001b[0m             \u001b[1;31m# compat pickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_wrapper\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m    145\u001b[0m                             \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m                             is_text=False)\n\u001b[0m\u001b[0;32m    147\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;31m# Python 3 and binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'trained_model.pkl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[1;34m(path, compression)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtry_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# noqa: E722\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mtry_read\u001b[1;34m(path, encoding)\u001b[0m\n\u001b[0;32m    176\u001b[0m                 return read_wrapper(\n\u001b[1;32m--> 177\u001b[1;33m                     lambda f: pc.load(f, encoding=encoding, compat=True))\n\u001b[0m\u001b[0;32m    178\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_wrapper\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m    145\u001b[0m                             \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m                             is_text=False)\n\u001b[0m\u001b[0;32m    147\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;31m# Python 3 and binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'trained_model.pkl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mtry_read\u001b[1;34m(path, encoding)\u001b[0m\n\u001b[0;32m    165\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mread_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpkl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# noqa: E722\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_wrapper\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m    145\u001b[0m                             \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m                             is_text=False)\n\u001b[0m\u001b[0;32m    147\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;31m# Python 3 and binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'trained_model.pkl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mtry_read\u001b[1;34m(path, encoding)\u001b[0m\n\u001b[0;32m    172\u001b[0m                 return read_wrapper(\n\u001b[1;32m--> 173\u001b[1;33m                     lambda f: pc.load(f, encoding=encoding, compat=False))\n\u001b[0m\u001b[0;32m    174\u001b[0m             \u001b[1;31m# compat pickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_wrapper\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m    145\u001b[0m                             \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m                             is_text=False)\n\u001b[0m\u001b[0;32m    147\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;31m# Python 3 and binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'trained_model.pkl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-919f7be2ba45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-d64393e06107>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_dataset_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'E:/VIT/RBL/ddi/dataset/DDICorpus/Test/test_for_ddi_extraction_task/DrugBank/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_training_data_from_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrained_model_pickle_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0my_pred\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[1;34m(path, compression)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# noqa: E722\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mPY3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mtry_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'latin1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mtry_read\u001b[1;34m(path, encoding)\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# noqa: E722\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m                 return read_wrapper(\n\u001b[1;32m--> 177\u001b[1;33m                     lambda f: pc.load(f, encoding=encoding, compat=True))\n\u001b[0m\u001b[0;32m    178\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtry_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_wrapper\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m    144\u001b[0m         f, fh = _get_handle(path, 'rb',\n\u001b[0;32m    145\u001b[0m                             \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m                             is_text=False)\n\u001b[0m\u001b[0;32m    147\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    428\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;31m# Python 3 and binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'trained_model.pkl'"
     ]
    }
   ],
   "source": [
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
